# A headless service to create DNS records.
apiVersion: v1
kind: Service
metadata:
  name: hdfs-namenode
  labels:
    app: hdfs-namenode
  annotations:
    # TODO: Deprecated. Replace tolerate-unready-endpoints with
    # v1.Service.PublishNotReadyAddresses.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - port: 8020
    name: fs
  - port: 50070
    name: http
  clusterIP: None
  selector:
    app: hdfs-namenode
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: hdfs-namenode
spec:
  selector:
    matchLabels:
      app: hdfs-namenode
  minAvailable: 1
---
# Provides a number of namenode start scripts that meet different needs.
# TODO: Support upgrade of metadata in case a new Hadoop version requires it.
apiVersion: v1
kind: ConfigMap
metadata:
  name: hdfs-namenode-start-scripts
data:
  # A bootstrap script which will start namenode daemons after conducting
  # optional metadata initialization steps. The metadata initialization
  # steps will take place in case the metadata dir is empty,
  # which will be the case only for the very first run. The specific steps
  # will differ depending on whether the namenode is active or standby.
  # We also assume, for the very first run, namenode-0 will be active and
  # namenode-1 will be standby as StatefulSet will launch namenode-0 first
  # and zookeeper will determine the sole namenode to be the active one.
  # For active namenode, the initialization steps will format the metadata,
  # zookeeper dir and journal node data entries.
  # For standby namenode, the initialization steps will simply receieve
  # the first batch of metadata updates from the journal node.
  format-and-run.sh: |
    #!/bin/bash -x
    _HDFS_BIN=$HADOOP_PREFIX/bin/hdfs
    if [[ ! -d /hadoop/dfs/name/current && "$MY_POD" = hdfs-namenode-0 ]]
    then
      $_HDFS_BIN --config $HADOOP_CONF_DIR namenode -format  \
          -nonInteractive hdfs-k8s || exit 1
      $_HDFS_BIN --config $HADOOP_CONF_DIR zkfc -formatZK  \
          -nonInteractive || exit 1
    elif [[ ! -d /hadoop/dfs/name/current && "$MY_POD" = hdfs-namenode-1 ]]
    then
      $_HDFS_BIN --config $HADOOP_CONF_DIR namenode -bootstrapStandby  \
          -nonInteractive || exit 1
    fi
    $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR start zkfc
    $_HDFS_BIN --config $HADOOP_CONF_DIR namenode

  # A start script that will just hang indefinitely. A user can then get
  # inside the pod and debug. Or a user can conduct a custom manual operations.
  do-nothing.sh: |
    #!/bin/bash -x
    /usr/bin/tail -f /var/log/dmesg

  # A start script that has user specified content. Can be used to conduct
  # ad-hoc operation as specified by a user.
  custom-run.sh: {{ .Values.customRunScript | quote }}
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: hdfs-namenode
spec:
  serviceName: "hdfs-namenode"
  # Create a size-2 set. The namenode DNS names will be
  # hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local
  # and hdfs-namenode-1.hdfs-namenode.default.svc.cluster.local
  replicas: 2
  template:
    metadata:
      labels:
        app: hdfs-namenode
    spec:
      {{- if .Values.hostNetworkEnabled }}
      # Use hostNetwork so datanodes connect to namenode without going through an overlay network
      # like weave. Otherwise, namenode fails to see physical IP address of datanodes.
      # Disabling this will break data locality as namenode will see pod virtual IPs and fails to
      # equate them with cluster node physical IPs associated with data nodes.
      # We currently disable this only for CI on minikube.
      hostNetwork: true
      hostPID: true
      dnsPolicy: ClusterFirstWithHostNet
      {{- else }}
      dnsPolicy: ClusterFirst
      {{- end }}
      {{- if .Values.global.affinityEnabled }}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - hfds-namenode
              topologyKey: "kubernetes.io/hostname"
      {{- end }}
      containers:
        # TODO: Support hadoop version as option.
        - name: hdfs-namenode
          image: uhopper/hadoop-namenode:2.7.2
          env:
            - name: HADOOP_CUSTOM_CONF_DIR
              value: /etc/hadoop-custom-conf
            - name: MULTIHOMED_NETWORK
              value: "0"
            # Used by the start script below.
            - name: MY_POD
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          command: ['/bin/sh', '-c']
          # The start script is provided by a config map.
          args:
            - /entrypoint.sh "/nn-start/{{ .Values.namenodeStartScript }}"
          ports:
          - containerPort: 8020
            name: fs
          - containerPort: 50070
            name: http
          volumeMounts:
            - name: metadatadir
              mountPath: /hadoop/dfs/name
            - name: nn-start
              mountPath: /nn-start
              readOnly: true
            - name: hdfs-config
              mountPath: /etc/hadoop-custom-conf
              readOnly: true
            {{- if .Values.kerberosEnabled }}
            - name: kerberos-config
              mountPath: /etc/krb5.conf
              subPath: {{ .Values.kerberosConfigFileName }}
              readOnly: true
            - name: kerberos-keytab-copy
              mountPath: /etc/security/
              readOnly: true
            {{- end }}
      {{- if .Values.kerberosEnabled }}
      initContainers:
        - name: copy-kerberos-keytab
          image: busybox:1.27.1
          command: ['sh', '-c']
          args:
            - cp /kerberos-keytabs/${MY_KERBEROS_NAME}*.keytab /kerberos-keytab-copy/hdfs.keytab
          env:
            - name: MY_KERBEROS_NAME
              valueFrom:
                fieldRef:
                {{- if .Values.hostNetworkEnabled }}
                  fieldPath: spec.nodeName
                {{- else }}
                  fieldPath: metadata.name
                {{- end }}
          volumeMounts:
            - name: kerberos-keytabs
              mountPath: /kerberos-keytabs
            - name: kerberos-keytab-copy
              mountPath: /kerberos-keytab-copy
      {{- end }}
      {{- if .Values.namenodePinningEnabled }}
      # Pin namenode pods to a set of nodes. You can label your node like below:
      #   $ kubectl label nodes YOUR-NODE hdfs-namenode-selector=hdfs-namenode
      nodeSelector:
        hdfs-namenode-selector: hdfs-namenode
      {{- end }}
      restartPolicy: Always
      volumes:
        - name: nn-start
          configMap:
            name: hdfs-namenode-start-scripts
            defaultMode: 0744
        - name: hdfs-config
          configMap:
            name: {{ .Values.hdfsConfigMap }}
        {{- if .Values.kerberosEnabled }}
        - name: kerberos-config
          configMap:
            name: {{ .Values.kerberosConfigMap }}
        - name: kerberos-keytabs
          secret:
            secretName: {{ .Values.kerberosKeytabsSecret }}
        - name: kerberos-keytab-copy
          emptyDir: {}
        {{- end }}
      {{- if .Values.podSecurityContext.enabled }}
      securityContext:
        runAsUser: {{ .Values.podSecurityContext.runAsUser }}
        fsGroup: {{ .Values.podSecurityContext.fsGroup }}
      {{- end }}
  volumeClaimTemplates:
  - metadata:
      name: metadatadir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {{ .Values.metadataVolumeSize }}
