


NAME:   my-hdfs
REVISION: 1
CHART: hdfs-0.1.0
USER-SUPPLIED VALUES:
global:
  dataNodeHostPath:
  - /mnt/sda1/hdfs-data0
  - /mnt/sda1/hdfs-data1
  defaultAffinityEnabled: false
  zookeeperQuorumSize: 1
hdfs-config-k8s.customHadoopConfig:
  coreSite:
    hadoop.http.authentication.simple.anonymous.allowed: "false"
    hadoop.http.authentication.type: kerberos
  hdfsSite:
    dfs.replication: 2
hdfs-namenode-k8s:
  hostNetworkEnabled: false
zookeeper:
  env:
    ZK_HEAP_SIZE: 100m
  replicaCount: 1
  resources:
    requests:
      memory: 100m

COMPUTED VALUES:
global:
  dataNodeHostPath:
  - /mnt/sda1/hdfs-data0
  - /mnt/sda1/hdfs-data1
  defaultAffinityEnabled: false
  journalnodeQuorumSize: 3
  jsvcEnabled: true
  kerberosConfigFileName: krb5.conf
  kerberosEnabled: false
  kerberosRealm: MYCOMPANY.COM
  namenodeHAEnabled: true
  podSecurityContext:
    enabled: false
    fsGroup: 1000
    runAsUser: 0
  zookeeperQuorumSize: 1
hdfs-client-k8s:
  global:
    dataNodeHostPath:
    - /mnt/sda1/hdfs-data0
    - /mnt/sda1/hdfs-data1
    defaultAffinityEnabled: false
    journalnodeQuorumSize: 3
    jsvcEnabled: true
    kerberosConfigFileName: krb5.conf
    kerberosEnabled: false
    kerberosRealm: MYCOMPANY.COM
    namenodeHAEnabled: true
    podSecurityContext:
      enabled: false
      fsGroup: 1000
      runAsUser: 0
    zookeeperQuorumSize: 1
hdfs-config-k8s:
  customHadoopConfig:
    coreSite: {}
    hdfsSite: {}
  global:
    dataNodeHostPath:
    - /mnt/sda1/hdfs-data0
    - /mnt/sda1/hdfs-data1
    defaultAffinityEnabled: false
    journalnodeQuorumSize: 3
    jsvcEnabled: true
    kerberosConfigFileName: krb5.conf
    kerberosEnabled: false
    kerberosRealm: MYCOMPANY.COM
    namenodeHAEnabled: true
    podSecurityContext:
      enabled: false
      fsGroup: 1000
      runAsUser: 0
    zookeeperQuorumSize: 1
hdfs-config-k8s.customHadoopConfig:
  coreSite:
    hadoop.http.authentication.simple.anonymous.allowed: "false"
    hadoop.http.authentication.type: kerberos
  hdfsSite:
    dfs.replication: 2
hdfs-datanode-k8s:
  affinity: {}
  global:
    dataNodeHostPath:
    - /mnt/sda1/hdfs-data0
    - /mnt/sda1/hdfs-data1
    defaultAffinityEnabled: false
    journalnodeQuorumSize: 3
    jsvcEnabled: true
    kerberosConfigFileName: krb5.conf
    kerberosEnabled: false
    kerberosRealm: MYCOMPANY.COM
    namenodeHAEnabled: true
    podSecurityContext:
      enabled: false
      fsGroup: 1000
      runAsUser: 0
    zookeeperQuorumSize: 1
  nodeSelector: {}
  tolerations: []
hdfs-journalnode-k8s:
  affinity: {}
  global:
    dataNodeHostPath:
    - /mnt/sda1/hdfs-data0
    - /mnt/sda1/hdfs-data1
    defaultAffinityEnabled: false
    journalnodeQuorumSize: 3
    jsvcEnabled: true
    kerberosConfigFileName: krb5.conf
    kerberosEnabled: false
    kerberosRealm: MYCOMPANY.COM
    namenodeHAEnabled: true
    podSecurityContext:
      enabled: false
      fsGroup: 1000
      runAsUser: 0
    zookeeperQuorumSize: 1
  nodeSelector: {}
  persistence:
    accessMode: ReadWriteOnce
    size: 20Gi
  tolerations: []
hdfs-krb5-k8s:
  image:
    pullPolicy: IfNotPresent
    repository: gcavalcante8808/krb5-server
    tag: latest
  persistence:
    accessMode: ReadWriteOnce
    size: 20Gi
  service:
    port: 88
    type: ClusterIP
hdfs-namenode-k8s:
  affinity: {}
  customRunScript: |
    #!/bin/bash -x
    echo Write your own script content!
    echo This message will disappear in 10 seconds.
    sleep 10
  global:
    dataNodeHostPath:
    - /mnt/sda1/hdfs-data0
    - /mnt/sda1/hdfs-data1
    defaultAffinityEnabled: false
    journalnodeQuorumSize: 3
    jsvcEnabled: true
    kerberosConfigFileName: krb5.conf
    kerberosEnabled: false
    kerberosRealm: MYCOMPANY.COM
    namenodeHAEnabled: true
    podSecurityContext:
      enabled: false
      fsGroup: 1000
      runAsUser: 0
    zookeeperQuorumSize: 1
  hostNetworkEnabled: false
  namenodeStartScript: format-and-run.sh
  nodeSelector: {}
  persistence:
    accessMode: ReadWriteOnce
    size: 100Gi
  tolerations: []
hdfs-simple-namenode-k8s:
  affinity: {}
  nameNodeHostPath: /hdfs-name
  nodeSelector: {}
  tolerations: []
tags:
  ha: true
  kerberos: false
  simple: false
zookeeper:
  affinity: {}
  env:
    JMXAUTH: "false"
    JMXDISABLE: "false"
    JMXPORT: 1099
    JMXSSL: "false"
    ZK_CLIENT_PORT: 2181
    ZK_ELECTION_PORT: 3888
    ZK_HEAP_SIZE: 100m
    ZK_INIT_LIMIT: 5
    ZK_LOG_LEVEL: INFO
    ZK_MAX_CLIENT_CNXNS: 60
    ZK_MAX_SESSION_TIMEOUT: 40000
    ZK_MIN_SESSION_TIMEOUT: 4000
    ZK_PURGE_INTERVAL: 0
    ZK_SERVER_PORT: 2888
    ZK_SNAP_RETAIN_COUNT: 3
    ZK_SYNC_LIMIT: 10
    ZK_TICK_TIME: 2000
  exporters:
    jmx:
      config:
        lowercaseOutputName: false
        rules:
        - name: zookeeper_$2
          pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+)><>(\w+)
        - labels:
            replicaId: $2
          name: zookeeper_$3
          pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+)><>(\w+)
        - labels:
            memberType: $3
            replicaId: $2
          name: zookeeper_$4
          pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
            name2=(\w+)><>(\w+)
        - labels:
            memberType: $3
            replicaId: $2
          name: zookeeper_$4_$5
          pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
            name2=(\w+), name3=(\w+)><>(\w+)
        startDelaySeconds: 30
      enabled: false
      env: {}
      image:
        pullPolicy: IfNotPresent
        repository: sscaling/jmx-prometheus-exporter
        tag: 0.3.0
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /metrics
          port: jmxxp
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 60
      path: /metrics
      ports:
        jmxxp:
          containerPort: 9404
          protocol: TCP
      readinessProbe:
        failureThreshold: 8
        httpGet:
          path: /metrics
          port: jmxxp
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 60
      resources: {}
    zookeeper:
      config:
        logLevel: info
        resetOnScrape: "true"
      enabled: false
      env: {}
      image:
        pullPolicy: IfNotPresent
        repository: josdotso/zookeeper-exporter
        tag: v1.1.2
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /metrics
          port: zookeeperxp
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 60
      path: /metrics
      ports:
        zookeeperxp:
          containerPort: 9141
          protocol: TCP
      readinessProbe:
        failureThreshold: 8
        httpGet:
          path: /metrics
          port: zookeeperxp
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 60
      resources: {}
  global:
    dataNodeHostPath:
    - /mnt/sda1/hdfs-data0
    - /mnt/sda1/hdfs-data1
    defaultAffinityEnabled: false
    journalnodeQuorumSize: 3
    jsvcEnabled: true
    kerberosConfigFileName: krb5.conf
    kerberosEnabled: false
    kerberosRealm: MYCOMPANY.COM
    namenodeHAEnabled: true
    podSecurityContext:
      enabled: false
      fsGroup: 1000
      runAsUser: 0
    zookeeperQuorumSize: 1
  image:
    pullPolicy: IfNotPresent
    repository: gcr.io/google_samples/k8szk
    tag: v3
  jobs:
    chroots:
      activeDeadlineSeconds: 300
      backoffLimit: 5
      completions: 1
      config:
        create: []
      enabled: false
      env: []
      parallelism: 1
      resources: {}
      restartPolicy: Never
  livenessProbe:
    exec:
      command:
      - zkOk.sh
    initialDelaySeconds: 20
  nodeSelector: {}
  persistence:
    accessMode: ReadWriteOnce
    enabled: true
    size: 5Gi
  podAnnotations: {}
  podDisruptionBudget:
    maxUnavailable: 1
  podLabels: {}
  ports:
    client:
      containerPort: 2181
      protocol: TCP
    election:
      containerPort: 3888
      protocol: TCP
    server:
      containerPort: 2888
      protocol: TCP
  readinessProbe:
    exec:
      command:
      - zkOk.sh
    initialDelaySeconds: 20
  replicaCount: 1
  resources:
    requests:
      memory: 100m
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  service:
    annotations: {}
    ports:
      client:
        port: 2181
        protocol: TCP
        targetPort: client
    type: ClusterIP
  terminationGracePeriodSeconds: 1800
  tolerations: []
  updateStrategy:
    type: OnDelete

HOOKS:
MANIFEST:

---
# Source: hdfs/charts/hdfs-config-k8s/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-hdfs-config
  labels:
    app: hdfs-client
    chart: hdfs-config-k8s-0.1.0
    release: my-hdfs
data:
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfs-k8s</value>
      </property>
      <property>
        <name>ha.zookeeper.quorum</name>
        <value>my-hdfs-zookeeper-0.my-hdfs-zookeeper-headless.default.svc.cluster.local:2181</value>
      </property>
    </configuration>
  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>dfs.nameservices</name>
        <value>hdfs-k8s</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.hdfs-k8s</name>
        <value>nn0,nn1</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.hdfs-k8s.nn0</name>
        <value>my-hdfs-namenode-0.my-hdfs-namenode.default.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.hdfs-k8s.nn1</name>
        <value>my-hdfs-namenode-1.my-hdfs-namenode.default.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.hdfs-k8s.nn0</name>
        <value>my-hdfs-namenode-0.my-hdfs-namenode.default.svc.cluster.local:50070</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.hdfs-k8s.nn1</name>
        <value>my-hdfs-namenode-1.my-hdfs-namenode.default.svc.cluster.local:50070</value>
      </property>
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://my-hdfs-journalnode-1.my-hdfs-journalnode.default.svc.cluster.local:8485;my-hdfs-journalnode-2.my-hdfs-journalnode.default.svc.cluster.local:8485;my-hdfs-journalnode-0.my-hdfs-journalnode.default.svc.cluster.local:8485/hdfs-k8s</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/hadoop/dfs/journal</value>
      </property>
      <property>
        <name>dfs.client.failover.proxy.provider.hdfs-k8s</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///hadoop/dfs/name</value>
      </property>
      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>/mnt/sda1/hdfs-data0,/mnt/sda1/hdfs-data1</value>
      </property>
    </configuration>
---
# Source: hdfs/charts/hdfs-datanode-k8s/templates/datanode-daemonset.yaml
# Provides datanode helper scripts.
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-hdfs-datanode-scripts
  labels:
    app: hdfs-datanode
    chart: hdfs-datanode-k8s-0.1.0
    release: my-hdfs
data:
  check-status.sh: |
    #!/usr/bin/env bash
    # Exit on error. Append "|| true" if you expect an error.
    set -o errexit
    # Exit on error inside any functions or subshells.
    set -o errtrace
    # Do not allow use of undefined vars. Use ${VAR:-} to use an undefined VAR
    set -o nounset
    # Catch an error in command pipes. e.g. mysqldump fails (but gzip succeeds)
    # in `mysqldump |gzip`
    set -o pipefail
    # Turn on traces, useful while debugging.
    set -o xtrace

    # Check if datanode registered with the namenode and got non-null cluster ID.
    _PORTS="50075 1006"
    _URL_PATH="jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo"
    _CLUSTER_ID=""
    for _PORT in $_PORTS; do
      _CLUSTER_ID+=$(curl -s http://localhost:${_PORT}/$_URL_PATH |  \
          grep ClusterId) || true
    done
    echo $_CLUSTER_ID | grep -q -v null
---
# Source: hdfs/charts/hdfs-namenode-k8s/templates/namenode-statefulset.yaml
# Provides namenode helper scripts. Most of them are start scripts
# that meet different needs.
# TODO: Support upgrade of metadata in case a new Hadoop version requires it.
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-hdfs-namenode-scripts
  labels:
    app: hdfs-namenode
    chart: hdfs-namenode-k8s-0.1.0
    release: my-hdfs
data:
  # A bootstrap script which will start namenode daemons after conducting
  # optional metadata initialization steps. The metadata initialization
  # steps will take place in case the metadata dir is empty,
  # which will be the case only for the very first run. The specific steps
  # will differ depending on whether the namenode is active or standby.
  # We also assume, for the very first run, namenode-0 will be active and
  # namenode-1 will be standby as StatefulSet will launch namenode-0 first
  # and zookeeper will determine the sole namenode to be the active one.
  # For active namenode, the initialization steps will format the metadata,
  # zookeeper dir and journal node data entries.
  # For standby namenode, the initialization steps will simply receieve
  # the first batch of metadata updates from the journal node.
  format-and-run.sh: |
    #!/usr/bin/env bash
    # Exit on error. Append "|| true" if you expect an error.
    set -o errexit
    # Exit on error inside any functions or subshells.
    set -o errtrace
    # Do not allow use of undefined vars. Use ${VAR:-} to use an undefined VAR
    set -o nounset
    # Catch an error in command pipes. e.g. mysqldump fails (but gzip succeeds)
    # in `mysqldump |gzip`
    set -o pipefail
    # Turn on traces, useful while debugging.
    set -o xtrace

    _HDFS_BIN=$HADOOP_PREFIX/bin/hdfs
    _METADATA_DIR=/hadoop/dfs/name/current
    if [[ "$MY_POD" = "$NAMENODE_POD_0" ]]; then
      if [[ ! -d $_METADATA_DIR ]]; then
          $_HDFS_BIN --config $HADOOP_CONF_DIR namenode -format  \
              -nonInteractive hdfs-k8s ||
              (rm -rf $_METADATA_DIR; exit 1)
      fi
      _ZKFC_FORMATTED=/hadoop/dfs/name/current/.hdfs-k8s-zkfc-formatted
      if [[ ! -f $_ZKFC_FORMATTED ]]; then
        _OUT=$($_HDFS_BIN --config $HADOOP_CONF_DIR zkfc -formatZK -nonInteractive 2>&1)
        # zkfc masks fatal exceptions and returns exit code 0
        (echo $_OUT | grep -q "FATAL") && exit 1
        touch $_ZKFC_FORMATTED
      fi
    elif [[ "$MY_POD" = "$NAMENODE_POD_1" ]]; then
      if [[ ! -d $_METADATA_DIR ]]; then
        $_HDFS_BIN --config $HADOOP_CONF_DIR namenode -bootstrapStandby  \
            -nonInteractive ||  \
            (rm -rf $_METADATA_DIR; exit 1)
      fi
    fi
    $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR start zkfc
    $_HDFS_BIN --config $HADOOP_CONF_DIR namenode

  # A start script that will just hang indefinitely. A user can then get
  # inside the pod and debug. Or a user can conduct a custom manual operations.
  do-nothing.sh: |
    #!/usr/bin/env bash
    tail -f /var/log/dmesg

  # A start script that has user specified content. Can be used to conduct
  # ad-hoc operation as specified by a user.
  custom-run.sh: "#!/bin/bash -x\necho Write your own script content!\necho This message will disappear in 10 seconds.\nsleep 10\n"
---
# Source: hdfs/charts/hdfs-journalnode-k8s/templates/journalnode-statefulset.yaml
# A headless service to create DNS records.
apiVersion: v1
kind: Service
metadata:
  name: my-hdfs-journalnode
  labels:
    app: hdfs-journalnode
    chart: hdfs-journalnode-k8s-0.1.0
    release: my-hdfs
  annotations:
    # TODO: Deprecated. Replace tolerate-unready-endpoints with
    # v1.Service.PublishNotReadyAddresses.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - port: 8485
    name: jn
  - port: 8480
    name: http
  clusterIP: None
  selector:
    app: hdfs-journalnode
    release: my-hdfs
---
# Source: hdfs/charts/hdfs-namenode-k8s/templates/namenode-statefulset.yaml
# A headless service to create DNS records.
apiVersion: v1
kind: Service
metadata:
  name: my-hdfs-namenode
  labels:
    app: hdfs-namenode
    chart: hdfs-namenode-k8s-0.1.0
    release: my-hdfs
  annotations:
    # TODO: Deprecated. Replace tolerate-unready-endpoints with
    # v1.Service.PublishNotReadyAddresses.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - port: 8020
    name: fs
  - port: 50070
    name: http
  clusterIP: None
  selector:
    app: hdfs-namenode
    release: my-hdfs
---
# Source: hdfs/charts/zookeeper/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-hdfs-zookeeper-headless
  labels:
    app: zookeeper
    chart: zookeeper-1.0.0
    release: my-hdfs
    heritage: Tiller
spec:
  clusterIP: None
  ports:
    - name: client
      port: 2181
      targetPort: 
      protocol: TCP
    - name: election
      port: 3888
      targetPort: 
      protocol: TCP
    - name: server
      port: 2888
      targetPort: 
      protocol: TCP
  selector:
    app: zookeeper
    release: my-hdfs
---
# Source: hdfs/charts/zookeeper/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-hdfs-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-1.0.0
    release: my-hdfs
    heritage: Tiller
  annotations:
spec:
  type: ClusterIP
  ports:
    - name: client
      port: 2181
      protocol: TCP
      targetPort: client
      
  selector:
    app: zookeeper
    release: my-hdfs
---
# Source: hdfs/charts/hdfs-datanode-k8s/templates/datanode-daemonset.yaml
# Deleting a daemonset may need some trick. See
# https://github.com/kubernetes/kubernetes/issues/33245#issuecomment-261250489
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: my-hdfs-datanode
  labels:
    app: hdfs-datanode
    chart: hdfs-datanode-k8s-0.1.0
    release: my-hdfs
spec:
  template:
    metadata:
      labels:
        app: hdfs-datanode
        release: my-hdfs
    spec:
      hostNetwork: true
      hostPID: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
        - name: datanode
          image: uhopper/hadoop-datanode:2.7.2
          env:
            - name: HADOOP_CUSTOM_CONF_DIR
              value: /etc/hadoop-custom-conf
            - name: MULTIHOMED_NETWORK
              value: "0"
          livenessProbe:
            exec:
              command:
                - /dn-scripts/check-status.sh
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            exec:
              command:
                - /dn-scripts/check-status.sh
            initialDelaySeconds: 60
            periodSeconds: 30
          securityContext:
            privileged: true
          volumeMounts:
            - name: dn-scripts
              mountPath: /dn-scripts
              readOnly: true
            - name: hdfs-config
              mountPath: /etc/hadoop-custom-conf
              readOnly: true
            - name: hdfs-data-0
              mountPath: /hadoop/dfs/data/0
            - name: hdfs-data-1
              mountPath: /hadoop/dfs/data/1
      restartPolicy: Always
      volumes:
        - name: dn-scripts
          configMap:
            name: my-hdfs-datanode-scripts
            defaultMode: 0744
        - name: hdfs-data-0
          hostPath:
            path: /mnt/sda1/hdfs-data0
        - name: hdfs-data-1
          hostPath:
            path: /mnt/sda1/hdfs-data1
        - name: hdfs-config
          configMap:
            name: my-hdfs-config
---
# Source: hdfs/charts/hdfs-client-k8s/templates/client-deployment.yaml
apiVersion: apps/v1
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-hdfs-client
  labels:
    app: hdfs-client
    chart: hdfs-client-k8s-0.1.0
    release: my-hdfs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hdfs-client
      release: my-hdfs
  template:
    metadata:
      labels:
        app: hdfs-client
        release: my-hdfs
    spec:
      containers:
        - name: hdfs-client
          image: uhopper/hadoop:2.7.2
          env:
            - name: HADOOP_CUSTOM_CONF_DIR
              value: /etc/hadoop-custom-conf
            - name: MULTIHOMED_NETWORK
              value: "0"
          command: ['/bin/sh', '-c']
          args:
            - /entrypoint.sh /usr/bin/tail -f /var/log/dmesg
          volumeMounts:
            - name: hdfs-config
              mountPath: /etc/hadoop-custom-conf
              readOnly: true
      restartPolicy: Always
      volumes:
        - name: hdfs-config
          configMap:
            name: my-hdfs-config
---
# Source: hdfs/charts/hdfs-journalnode-k8s/templates/journalnode-statefulset.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: my-hdfs-journalnode
  labels:
    app: hdfs-journalnode
    chart: hdfs-journalnode-k8s-0.1.0
    release: my-hdfs
spec:
  serviceName: my-hdfs-journalnode
  replicas: 3
  template:
    metadata:
      labels:
        app: hdfs-journalnode
        release: my-hdfs
    spec:
      containers:
        - name: hdfs-journalnode
          image: uhopper/hadoop-namenode:2.7.2
          env:
            - name: HADOOP_CUSTOM_CONF_DIR
              value: /etc/hadoop-custom-conf
          command: ["/entrypoint.sh"]
          args: ["/opt/hadoop-2.7.2/bin/hdfs", "--config", "/etc/hadoop", "journalnode"]
          ports:
          - containerPort: 8485
            name: jn
          - containerPort: 8480
            name: http
          volumeMounts:
            # Mount a subpath of the volume so that the journal subdir would be
            # a brand new empty dir. This way, we won't get affected by
            # existing files in the volume top dir.
            - name: editdir
              mountPath: /hadoop/dfs/journal
              subPath: journal
            - name: editdir
              mountPath: /hadoop/dfs/name
              subPath: name
            - name: hdfs-config
              mountPath: /etc/hadoop-custom-conf
              readOnly: true
      restartPolicy: Always
      volumes:
        - name: hdfs-config
          configMap:
            name: my-hdfs-config
  volumeClaimTemplates:
    - metadata:
        name: editdir
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "20Gi"
---
# Source: hdfs/charts/hdfs-namenode-k8s/templates/namenode-statefulset.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: my-hdfs-namenode
  labels:
    app: hdfs-namenode
    chart: hdfs-namenode-k8s-0.1.0
    release: my-hdfs
spec:
  serviceName: my-hdfs-namenode
  replicas: 2
  template:
    metadata:
      labels:
        app: hdfs-namenode
        release: my-hdfs
    spec:
      dnsPolicy: ClusterFirst
      containers:
        # TODO: Support hadoop version as option.
        - name: hdfs-namenode
          image: uhopper/hadoop-namenode:2.7.2
          env:
            - name: HADOOP_CUSTOM_CONF_DIR
              value: /etc/hadoop-custom-conf
            - name: MULTIHOMED_NETWORK
              value: "0"
            # Used by the start script below.
            - name: MY_POD
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NAMENODE_POD_0
              value: my-hdfs-namenode-0
            - name: NAMENODE_POD_1
              value: my-hdfs-namenode-1
          command: ['/bin/sh', '-c']
          # The start script is provided by a config map.
          args:
            - /entrypoint.sh "/nn-scripts/format-and-run.sh"
          ports:
          - containerPort: 8020
            name: fs
          - containerPort: 50070
            name: http
          volumeMounts:
            - name: nn-scripts
              mountPath: /nn-scripts
              readOnly: true
            # Mount a subpath of the volume so that the name subdir would be a
            # brand new empty dir. This way, we won't get affected by existing
            # files in the volume top dir.
            - name: metadatadir
              mountPath: /hadoop/dfs/name
              subPath: name
            - name: hdfs-config
              mountPath: /etc/hadoop-custom-conf
              readOnly: true
      restartPolicy: Always
      volumes:
        - name: nn-scripts
          configMap:
            name: my-hdfs-namenode-scripts
            defaultMode: 0744
        - name: hdfs-config
          configMap:
            name: my-hdfs-config
  volumeClaimTemplates:
    - metadata:
        name: metadatadir
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "100Gi"
---
# Source: hdfs/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: my-hdfs-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-1.0.0
    release: my-hdfs
    heritage: Tiller
    component: server
spec:
  serviceName: my-hdfs-zookeeper-headless
  replicas: 1
  terminationGracePeriodSeconds: 1800
  selector:
    matchLabels:
      app: zookeeper
      release: my-hdfs
      component: server
  updateStrategy:
    type: OnDelete
    
  template:
    metadata:
      labels:
        app: zookeeper
        release: my-hdfs
        component: server
      annotations:
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        
      containers:

        - name: zookeeper
          image: "gcr.io/google_samples/k8szk:v3"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -xec
            - zkGenConfig.sh && exec zkServer.sh start-foreground
          ports:
            - name: client
              containerPort: 2181
              protocol: TCP
              
            - name: election
              containerPort: 3888
              protocol: TCP
              
            - name: server
              containerPort: 2888
              protocol: TCP
              
          livenessProbe:
            exec:
              command:
              - zkOk.sh
            initialDelaySeconds: 20
            
          readinessProbe:
            exec:
              command:
              - zkOk.sh
            initialDelaySeconds: 20
            
          env:
            - name: ZK_REPLICAS
              value: "1"
            - name: JMXAUTH
              value: "false"
            - name: JMXDISABLE
              value: "false"
            - name: JMXPORT
              value: "1099"
            - name: JMXSSL
              value: "false"
            - name: ZK_CLIENT_PORT
              value: "2181"
            - name: ZK_ELECTION_PORT
              value: "3888"
            - name: ZK_HEAP_SIZE
              value: "100m"
            - name: ZK_INIT_LIMIT
              value: "5"
            - name: ZK_LOG_LEVEL
              value: "INFO"
            - name: ZK_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZK_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZK_MIN_SESSION_TIMEOUT
              value: "4000"
            - name: ZK_PURGE_INTERVAL
              value: "0"
            - name: ZK_SERVER_PORT
              value: "2888"
            - name: ZK_SNAP_RETAIN_COUNT
              value: "3"
            - name: ZK_SYNC_LIMIT
              value: "10"
            - name: ZK_TICK_TIME
              value: "2000"
          resources:
            requests:
              memory: 100m
            
          volumeMounts:
            - name: data
              mountPath: /var/lib/zookeeper

  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "5Gi"
---
# Source: hdfs/charts/hdfs-journalnode-k8s/templates/journalnode-statefulset.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-hdfs-journalnode
  labels:
    app: hdfs-journalnode
    chart: hdfs-journalnode-k8s-0.1.0
    release: my-hdfs
spec:
  selector:
    matchLabels:
      app: hdfs-journalnode
      release: my-hdfs
  minAvailable: 2
---
# Source: hdfs/charts/hdfs-namenode-k8s/templates/namenode-statefulset.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-hdfs-namenode
  labels:
    app: hdfs-namenode
    chart: hdfs-namenode-k8s-0.1.0
    release: my-hdfs
spec:
  selector:
    matchLabels:
      app: hdfs-namenode
      release: my-hdfs
  minAvailable: 1
---
# Source: hdfs/charts/zookeeper/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: my-hdfs-zookeeper
  labels:
    app: zookeeper
    chart: zookeeper-1.0.0
    release: my-hdfs
    heritage: Tiller
    component: server
spec:
  selector:
    matchLabels:
      app: zookeeper
      release: my-hdfs
      component: server
  maxUnavailable: 1
